{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c84fbec2-e7c0-4178-8fa1-b52aa8b43218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Text Here Tokenization is the process of splitting the input and output texts into smaller units that can be processed by the LLM AI models. Tokens can be words, characters, subwords, or symbols, depending on the type and the size of the model. Tokenization can help the model to handle different languages, vocabularies, and formats, and to reduce the computational and memory costs. Tokenization can also affect the quality and the diversity of the generated texts, by influencing the meaning and the context of the tokens. Tokenization can be done using different methods, such as rule-based, statistical, or neural, depending on the complexity and the variability of the texts.  OpenAI and Azure OpenAI uses a subword tokenization method called \"Byte-Pair Encoding (BPE)\" for its GPT-based models. BPE is a method that merges the most frequently occurring pairs of characters or bytes into a single token, until a certain number of tokens or a vocabulary size is reached. BPE can help the model to handle rare or unseen words, and to create more compact and consistent representations of the texts. BPE can also allow the model to generate new words or tokens, by combining existing ones. Example text excerpt from: https://learn.microsoft.com/en-us/semantic-kernel/concepts-ai/tokens \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3404, 2065, 374, 279, 1920, 315, 45473, 279, 1988, 323, 2612, 22755, 1139, 9333, 8316, 430, 649, 387, 15590, 555, 279, 445, 11237, 15592, 4211, 13, 59266, 649, 387, 4339, 11, 5885, 11, 1207, 5880, 11, 477, 18210, 11, 11911, 389, 279, 955, 323, 279, 1404, 315, 279, 1646, 13, 9857, 2065, 649, 1520, 279, 1646, 311, 3790, 2204, 15823, 11, 24757, 1299, 552, 11, 323, 20447, 11, 323, 311, 8108, 279, 55580, 323, 5044, 7194, 13, 9857, 2065, 649, 1101, 7958, 279, 4367, 323, 279, 20057, 315, 279, 8066, 22755, 11, 555, 66700, 279, 7438, 323, 279, 2317, 315, 279, 11460, 13, 9857, 2065, 649, 387, 2884, 1701, 2204, 5528, 11, 1778, 439, 6037, 6108, 11, 29564, 11, 477, 30828, 11, 11911, 389, 279, 23965, 323, 279, 54709, 315, 279, 22755, 13, 220, 5377, 15836, 323, 35219, 5377, 15836, 5829, 264, 1207, 1178, 4037, 2065, 1749, 2663, 330, 7300, 9483, 1334, 30430, 320, 33, 1777, 10143, 369, 1202, 480, 2898, 6108, 4211, 13, 426, 1777, 374, 264, 1749, 430, 82053, 279, 1455, 14134, 31965, 13840, 315, 5885, 477, 5943, 1139, 264, 3254, 4037, 11, 3156, 264, 3738, 1396, 315, 11460, 477, 264, 36018, 1404, 374, 8813, 13, 426, 1777, 649, 1520, 279, 1646, 311, 3790, 9024, 477, 64233, 4339, 11, 323, 311, 1893, 810, 17251, 323, 13263, 44713, 315, 279, 22755, 13, 426, 1777, 649, 1101, 2187, 279, 1646, 311, 7068, 502, 4339, 477, 11460, 11, 555, 35271, 6484, 6305, 13, 13688, 1495, 50565, 505, 25, 3788, 1129, 12964, 28719, 916, 13920, 26766, 14, 48958, 12934, 5571, 32336, 58871, 12, 2192, 5640, 9912, 220]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "user_input= input(\"Enter Text Here\")\n",
    "\n",
    "tokens=tokenizer.encode(user_input)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa59a0c-f35f-4ad3-ae7c-0e4ad9d4da88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization is the process of splitting the input and output texts into smaller units that can be processed by the LLM AI models. Tokens can be words, characters, subwords, or symbols, depending on the type and the size of the model. Tokenization can help the model to handle different languages, vocabularies, and formats, and to reduce the computational and memory costs. Tokenization can also affect the quality and the diversity of the generated texts, by influencing the meaning and the context of the tokens. Tokenization can be done using different methods, such as rule-based, statistical, or neural, depending on the complexity and the variability of the texts.  OpenAI and Azure OpenAI uses a subword tokenization method called \"Byte-Pair Encoding (BPE)\" for its GPT-based models. BPE is a method that merges the most frequently occurring pairs of characters or bytes into a single token, until a certain number of tokens or a vocabulary size is reached. BPE can help the model to handle rare or unseen words, and to create more compact and consistent representations of the texts. BPE can also allow the model to generate new words or tokens, by combining existing ones. Example text excerpt from: https://learn.microsoft.com/en-us/semantic-kernel/concepts-ai/tokens \n"
     ]
    }
   ],
   "source": [
    "decode = tokenizer.decode(tokens)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63930a07-a3ea-4976-b437-6a0ede76da55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Tokenization is the process of splitting the input and output texts into smaller units that can be processed by the LLM AI models. Tokens can be words, characters, subwords, or symbols, depending on the type and the size of the model. Tokenization can help the model to handle different languages, vocabularies, and formats, and to reduce the computational and memory costs. Tokenization can also affect the quality and the diversity of the generated texts, by influencing the meaning and the context of the tokens. Tokenization can be done using different methods, such as rule-based, statistical, or neural, depending on the complexity and the variability of the texts.  OpenAI and Azure OpenAI uses a subword tokenization method called \"Byte-Pair Encoding (BPE)\" for its GPT-based models. BPE is a method that merges the most frequently occurring pairs of characters or bytes into a single token, until a certain number of tokens or a vocabulary size is reached. BPE can help the model to handle rare or unseen words, and to create more compact and consistent representations of the texts. BPE can also allow the model to generate new words or tokens, by combining existing ones. Example text excerpt from: https://learn.microsoft.com/en-us/semantic-kernel/concepts-ai/tokens \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3404, 2065, 374, 279, 1920, 315, 45473, 279, 1988, 323, 2612, 22755, 1139, 9333, 8316, 430, 649, 387, 15590, 555, 279, 445, 11237, 15592, 4211, 13, 59266, 649, 387, 4339, 11, 5885, 11, 1207, 5880, 11, 477, 18210, 11, 11911, 389, 279, 955, 323, 279, 1404, 315, 279, 1646, 13, 9857, 2065, 649, 1520, 279, 1646, 311, 3790, 2204, 15823, 11, 24757, 1299, 552, 11, 323, 20447, 11, 323, 311, 8108, 279, 55580, 323, 5044, 7194, 13, 9857, 2065, 649, 1101, 7958, 279, 4367, 323, 279, 20057, 315, 279, 8066, 22755, 11, 555, 66700, 279, 7438, 323, 279, 2317, 315, 279, 11460, 13, 9857, 2065, 649, 387, 2884, 1701, 2204, 5528, 11, 1778, 439, 6037, 6108, 11, 29564, 11, 477, 30828, 11, 11911, 389, 279, 23965, 323, 279, 54709, 315, 279, 22755, 13, 220, 5377, 15836, 323, 35219, 5377, 15836, 5829, 264, 1207, 1178, 4037, 2065, 1749, 2663, 330, 7300, 9483, 1334, 30430, 320, 33, 1777, 10143, 369, 1202, 480, 2898, 6108, 4211, 13, 426, 1777, 374, 264, 1749, 430, 82053, 279, 1455, 14134, 31965, 13840, 315, 5885, 477, 5943, 1139, 264, 3254, 4037, 11, 3156, 264, 3738, 1396, 315, 11460, 477, 264, 36018, 1404, 374, 8813, 13, 426, 1777, 649, 1520, 279, 1646, 311, 3790, 9024, 477, 64233, 4339, 11, 323, 311, 1893, 810, 17251, 323, 13263, 44713, 315, 279, 22755, 13, 426, 1777, 649, 1101, 2187, 279, 1646, 311, 7068, 502, 4339, 477, 11460, 11, 555, 35271, 6484, 6305, 13, 13688, 1495, 50565, 505, 25, 3788, 1129, 12964, 28719, 916, 13920, 26766, 14, 48958, 12934, 5571, 32336, 58871, 12, 2192, 5640, 9912, 220]\n",
      "[b'Token', b'ization', b' is', b' the', b' process', b' of', b' splitting', b' the', b' input', b' and', b' output', b' texts', b' into', b' smaller', b' units', b' that', b' can', b' be', b' processed', b' by', b' the', b' L', b'LM', b' AI', b' models', b'.', b' Tokens', b' can', b' be', b' words', b',', b' characters', b',', b' sub', b'words', b',', b' or', b' symbols', b',', b' depending', b' on', b' the', b' type', b' and', b' the', b' size', b' of', b' the', b' model', b'.', b' Token', b'ization', b' can', b' help', b' the', b' model', b' to', b' handle', b' different', b' languages', b',', b' vocab', b'ular', b'ies', b',', b' and', b' formats', b',', b' and', b' to', b' reduce', b' the', b' computational', b' and', b' memory', b' costs', b'.', b' Token', b'ization', b' can', b' also', b' affect', b' the', b' quality', b' and', b' the', b' diversity', b' of', b' the', b' generated', b' texts', b',', b' by', b' influencing', b' the', b' meaning', b' and', b' the', b' context', b' of', b' the', b' tokens', b'.', b' Token', b'ization', b' can', b' be', b' done', b' using', b' different', b' methods', b',', b' such', b' as', b' rule', b'-based', b',', b' statistical', b',', b' or', b' neural', b',', b' depending', b' on', b' the', b' complexity', b' and', b' the', b' variability', b' of', b' the', b' texts', b'.', b' ', b' Open', b'AI', b' and', b' Azure', b' Open', b'AI', b' uses', b' a', b' sub', b'word', b' token', b'ization', b' method', b' called', b' \"', b'Byte', b'-P', b'air', b' Encoding', b' (', b'B', b'PE', b')\"', b' for', b' its', b' G', b'PT', b'-based', b' models', b'.', b' B', b'PE', b' is', b' a', b' method', b' that', b' merges', b' the', b' most', b' frequently', b' occurring', b' pairs', b' of', b' characters', b' or', b' bytes', b' into', b' a', b' single', b' token', b',', b' until', b' a', b' certain', b' number', b' of', b' tokens', b' or', b' a', b' vocabulary', b' size', b' is', b' reached', b'.', b' B', b'PE', b' can', b' help', b' the', b' model', b' to', b' handle', b' rare', b' or', b' unseen', b' words', b',', b' and', b' to', b' create', b' more', b' compact', b' and', b' consistent', b' representations', b' of', b' the', b' texts', b'.', b' B', b'PE', b' can', b' also', b' allow', b' the', b' model', b' to', b' generate', b' new', b' words', b' or', b' tokens', b',', b' by', b' combining', b' existing', b' ones', b'.', b' Example', b' text', b' excerpt', b' from', b':', b' https', b'://', b'learn', b'.microsoft', b'.com', b'/en', b'-us', b'/', b'semantic', b'-k', b'ernel', b'/con', b'cepts', b'-', b'ai', b'/t', b'okens', b' ']\n"
     ]
    }
   ],
   "source": [
    "user_input =input(\"\")\n",
    "tokens= tokenizer.encode(user_input)\n",
    "decode_to_bytes =tokenizer.decode_tokens_bytes(tokens)\n",
    "\n",
    "print(tokens)\n",
    "print(decode_to_bytes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4752498d-6913-468a-b90a-01cabc1ffe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;47;1mToken\u001b[0m\u001b[0;42;1mization\u001b[0m\u001b[0;43;1m is\u001b[0m\u001b[0;44;1m the\u001b[0m\u001b[0;46;1m process\u001b[0m\u001b[0;45;1m of\u001b[0m\u001b[0;47;1m splitting\u001b[0m\u001b[0;42;1m the\u001b[0m\u001b[0;43;1m input\u001b[0m\u001b[0;44;1m and\u001b[0m\u001b[0;46;1m output\u001b[0m\u001b[0;45;1m texts\u001b[0m\u001b[0;47;1m into\u001b[0m\u001b[0;42;1m smaller\u001b[0m\u001b[0;43;1m units\u001b[0m\u001b[0;44;1m that\u001b[0m\u001b[0;46;1m can\u001b[0m\u001b[0;45;1m be\u001b[0m\u001b[0;47;1m processed\u001b[0m\u001b[0;42;1m by\u001b[0m\u001b[0;43;1m the\u001b[0m\u001b[0;44;1m L\u001b[0m\u001b[0;46;1mLM\u001b[0m\u001b[0;45;1m AI\u001b[0m\u001b[0;47;1m models\u001b[0m\u001b[0;42;1m.\u001b[0m\u001b[0;43;1m Tokens\u001b[0m\u001b[0;44;1m can\u001b[0m\u001b[0;46;1m be\u001b[0m\u001b[0;45;1m words\u001b[0m\u001b[0;47;1m,\u001b[0m\u001b[0;42;1m characters\u001b[0m\u001b[0;43;1m,\u001b[0m\u001b[0;44;1m sub\u001b[0m\u001b[0;46;1mwords\u001b[0m\u001b[0;45;1m,\u001b[0m\u001b[0;47;1m or\u001b[0m\u001b[0;42;1m symbols\u001b[0m\u001b[0;43;1m,\u001b[0m\u001b[0;44;1m depending\u001b[0m\u001b[0;46;1m on\u001b[0m\u001b[0;45;1m the\u001b[0m\u001b[0;47;1m type\u001b[0m\u001b[0;42;1m and\u001b[0m\u001b[0;43;1m the\u001b[0m\u001b[0;44;1m size\u001b[0m\u001b[0;46;1m of\u001b[0m\u001b[0;45;1m the\u001b[0m\u001b[0;47;1m model\u001b[0m\u001b[0;42;1m.\u001b[0m\u001b[0;43;1m Token\u001b[0m\u001b[0;44;1mization\u001b[0m\u001b[0;46;1m can\u001b[0m\u001b[0;45;1m help\u001b[0m\u001b[0;47;1m the\u001b[0m\u001b[0;42;1m model\u001b[0m\u001b[0;43;1m to\u001b[0m\u001b[0;44;1m handle\u001b[0m\u001b[0;46;1m different\u001b[0m\u001b[0;45;1m languages\u001b[0m\u001b[0;47;1m,\u001b[0m\u001b[0;42;1m vocab\u001b[0m\u001b[0;43;1mular\u001b[0m\u001b[0;44;1mies\u001b[0m\u001b[0;46;1m,\u001b[0m\u001b[0;45;1m and\u001b[0m\u001b[0;47;1m formats\u001b[0m\u001b[0;42;1m,\u001b[0m\u001b[0;43;1m and\u001b[0m\u001b[0;44;1m to\u001b[0m\u001b[0;46;1m reduce\u001b[0m\u001b[0;45;1m the\u001b[0m\u001b[0;47;1m computational\u001b[0m\u001b[0;42;1m and\u001b[0m\u001b[0;43;1m memory\u001b[0m\u001b[0;44;1m costs\u001b[0m\u001b[0;46;1m.\u001b[0m\u001b[0;45;1m Token\u001b[0m\u001b[0;47;1mization\u001b[0m\u001b[0;42;1m can\u001b[0m\u001b[0;43;1m also\u001b[0m\u001b[0;44;1m affect\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m quality\u001b[0m\u001b[0;47;1m and\u001b[0m\u001b[0;42;1m the\u001b[0m\u001b[0;43;1m diversity\u001b[0m\u001b[0;44;1m of\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m generated\u001b[0m\u001b[0;47;1m texts\u001b[0m\u001b[0;42;1m,\u001b[0m\u001b[0;43;1m by\u001b[0m\u001b[0;44;1m influencing\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m meaning\u001b[0m\u001b[0;47;1m and\u001b[0m\u001b[0;42;1m the\u001b[0m\u001b[0;43;1m context\u001b[0m\u001b[0;44;1m of\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m tokens\u001b[0m\u001b[0;47;1m.\u001b[0m\u001b[0;42;1m Token\u001b[0m\u001b[0;43;1mization\u001b[0m\u001b[0;44;1m can\u001b[0m\u001b[0;46;1m be\u001b[0m\u001b[0;45;1m done\u001b[0m\u001b[0;47;1m using\u001b[0m\u001b[0;42;1m different\u001b[0m\u001b[0;43;1m methods\u001b[0m\u001b[0;44;1m,\u001b[0m\u001b[0;46;1m such\u001b[0m\u001b[0;45;1m as\u001b[0m\u001b[0;47;1m rule\u001b[0m\u001b[0;42;1m-based\u001b[0m\u001b[0;43;1m,\u001b[0m\u001b[0;44;1m statistical\u001b[0m\u001b[0;46;1m,\u001b[0m\u001b[0;45;1m or\u001b[0m\u001b[0;47;1m neural\u001b[0m\u001b[0;42;1m,\u001b[0m\u001b[0;43;1m depending\u001b[0m\u001b[0;44;1m on\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m complexity\u001b[0m\u001b[0;47;1m and\u001b[0m\u001b[0;42;1m the\u001b[0m\u001b[0;43;1m variability\u001b[0m\u001b[0;44;1m of\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m texts\u001b[0m\u001b[0;47;1m.\u001b[0m\u001b[0;42;1m \u001b[0m\u001b[0;43;1m Open\u001b[0m\u001b[0;44;1mAI\u001b[0m\u001b[0;46;1m and\u001b[0m\u001b[0;45;1m Azure\u001b[0m\u001b[0;47;1m Open\u001b[0m\u001b[0;42;1mAI\u001b[0m\u001b[0;43;1m uses\u001b[0m\u001b[0;44;1m a\u001b[0m\u001b[0;46;1m sub\u001b[0m\u001b[0;45;1mword\u001b[0m\u001b[0;47;1m token\u001b[0m\u001b[0;42;1mization\u001b[0m\u001b[0;43;1m method\u001b[0m\u001b[0;44;1m called\u001b[0m\u001b[0;46;1m \"\u001b[0m\u001b[0;45;1mByte\u001b[0m\u001b[0;47;1m-P\u001b[0m\u001b[0;42;1mair\u001b[0m\u001b[0;43;1m Encoding\u001b[0m\u001b[0;44;1m (\u001b[0m\u001b[0;46;1mB\u001b[0m\u001b[0;45;1mPE\u001b[0m\u001b[0;47;1m)\"\u001b[0m\u001b[0;42;1m for\u001b[0m\u001b[0;43;1m its\u001b[0m\u001b[0;44;1m G\u001b[0m\u001b[0;46;1mPT\u001b[0m\u001b[0;45;1m-based\u001b[0m\u001b[0;47;1m models\u001b[0m\u001b[0;42;1m.\u001b[0m\u001b[0;43;1m B\u001b[0m\u001b[0;44;1mPE\u001b[0m\u001b[0;46;1m is\u001b[0m\u001b[0;45;1m a\u001b[0m\u001b[0;47;1m method\u001b[0m\u001b[0;42;1m that\u001b[0m\u001b[0;43;1m merges\u001b[0m\u001b[0;44;1m the\u001b[0m\u001b[0;46;1m most\u001b[0m\u001b[0;45;1m frequently\u001b[0m\u001b[0;47;1m occurring\u001b[0m\u001b[0;42;1m pairs\u001b[0m\u001b[0;43;1m of\u001b[0m\u001b[0;44;1m characters\u001b[0m\u001b[0;46;1m or\u001b[0m\u001b[0;45;1m bytes\u001b[0m\u001b[0;47;1m into\u001b[0m\u001b[0;42;1m a\u001b[0m\u001b[0;43;1m single\u001b[0m\u001b[0;44;1m token\u001b[0m\u001b[0;46;1m,\u001b[0m\u001b[0;45;1m until\u001b[0m\u001b[0;47;1m a\u001b[0m\u001b[0;42;1m certain\u001b[0m\u001b[0;43;1m number\u001b[0m\u001b[0;44;1m of\u001b[0m\u001b[0;46;1m tokens\u001b[0m\u001b[0;45;1m or\u001b[0m\u001b[0;47;1m a\u001b[0m\u001b[0;42;1m vocabulary\u001b[0m\u001b[0;43;1m size\u001b[0m\u001b[0;44;1m is\u001b[0m\u001b[0;46;1m reached\u001b[0m\u001b[0;45;1m.\u001b[0m\u001b[0;47;1m B\u001b[0m\u001b[0;42;1mPE\u001b[0m\u001b[0;43;1m can\u001b[0m\u001b[0;44;1m help\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m model\u001b[0m\u001b[0;47;1m to\u001b[0m\u001b[0;42;1m handle\u001b[0m\u001b[0;43;1m rare\u001b[0m\u001b[0;44;1m or\u001b[0m\u001b[0;46;1m unseen\u001b[0m\u001b[0;45;1m words\u001b[0m\u001b[0;47;1m,\u001b[0m\u001b[0;42;1m and\u001b[0m\u001b[0;43;1m to\u001b[0m\u001b[0;44;1m create\u001b[0m\u001b[0;46;1m more\u001b[0m\u001b[0;45;1m compact\u001b[0m\u001b[0;47;1m and\u001b[0m\u001b[0;42;1m consistent\u001b[0m\u001b[0;43;1m representations\u001b[0m\u001b[0;44;1m of\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m texts\u001b[0m\u001b[0;47;1m.\u001b[0m\u001b[0;42;1m B\u001b[0m\u001b[0;43;1mPE\u001b[0m\u001b[0;44;1m can\u001b[0m\u001b[0;46;1m also\u001b[0m\u001b[0;45;1m allow\u001b[0m\u001b[0;47;1m the\u001b[0m\u001b[0;42;1m model\u001b[0m\u001b[0;43;1m to\u001b[0m\u001b[0;44;1m generate\u001b[0m\u001b[0;46;1m new\u001b[0m\u001b[0;45;1m words\u001b[0m\u001b[0;47;1m or\u001b[0m\u001b[0;42;1m tokens\u001b[0m\u001b[0;43;1m,\u001b[0m\u001b[0;44;1m by\u001b[0m\u001b[0;46;1m combining\u001b[0m\u001b[0;45;1m existing\u001b[0m\u001b[0;47;1m ones\u001b[0m\u001b[0;42;1m.\u001b[0m\u001b[0;43;1m Example\u001b[0m\u001b[0;44;1m text\u001b[0m\u001b[0;46;1m excerpt\u001b[0m\u001b[0;45;1m from\u001b[0m\u001b[0;47;1m:\u001b[0m\u001b[0;42;1m https\u001b[0m\u001b[0;43;1m://\u001b[0m\u001b[0;44;1mlearn\u001b[0m\u001b[0;46;1m.microsoft\u001b[0m\u001b[0;45;1m.com\u001b[0m\u001b[0;47;1m/en\u001b[0m\u001b[0;42;1m-us\u001b[0m\u001b[0;43;1m/\u001b[0m\u001b[0;44;1msemantic\u001b[0m\u001b[0;46;1m-k\u001b[0m\u001b[0;45;1mernel\u001b[0m\u001b[0;47;1m/con\u001b[0m\u001b[0;42;1mcepts\u001b[0m\u001b[0;43;1m-\u001b[0m\u001b[0;44;1mai\u001b[0m\u001b[0;46;1m/t\u001b[0m\u001b[0;45;1mokens\u001b[0m\u001b[0;47;1m \u001b[0m\n",
      "\n",
      "['Token', 'ization', ' is', ' the', ' process', ' of', ' splitting', ' the', ' input', ' and', ' output', ' texts', ' into', ' smaller', ' units', ' that', ' can', ' be', ' processed', ' by', ' the', ' L', 'LM', ' AI', ' models', '.', ' Tokens', ' can', ' be', ' words', ',', ' characters', ',', ' sub', 'words', ',', ' or', ' symbols', ',', ' depending', ' on', ' the', ' type', ' and', ' the', ' size', ' of', ' the', ' model', '.', ' Token', 'ization', ' can', ' help', ' the', ' model', ' to', ' handle', ' different', ' languages', ',', ' vocab', 'ular', 'ies', ',', ' and', ' formats', ',', ' and', ' to', ' reduce', ' the', ' computational', ' and', ' memory', ' costs', '.', ' Token', 'ization', ' can', ' also', ' affect', ' the', ' quality', ' and', ' the', ' diversity', ' of', ' the', ' generated', ' texts', ',', ' by', ' influencing', ' the', ' meaning', ' and', ' the', ' context', ' of', ' the', ' tokens', '.', ' Token', 'ization', ' can', ' be', ' done', ' using', ' different', ' methods', ',', ' such', ' as', ' rule', '-based', ',', ' statistical', ',', ' or', ' neural', ',', ' depending', ' on', ' the', ' complexity', ' and', ' the', ' variability', ' of', ' the', ' texts', '.', ' ', ' Open', 'AI', ' and', ' Azure', ' Open', 'AI', ' uses', ' a', ' sub', 'word', ' token', 'ization', ' method', ' called', ' \"', 'Byte', '-P', 'air', ' Encoding', ' (', 'B', 'PE', ')\"', ' for', ' its', ' G', 'PT', '-based', ' models', '.', ' B', 'PE', ' is', ' a', ' method', ' that', ' merges', ' the', ' most', ' frequently', ' occurring', ' pairs', ' of', ' characters', ' or', ' bytes', ' into', ' a', ' single', ' token', ',', ' until', ' a', ' certain', ' number', ' of', ' tokens', ' or', ' a', ' vocabulary', ' size', ' is', ' reached', '.', ' B', 'PE', ' can', ' help', ' the', ' model', ' to', ' handle', ' rare', ' or', ' unseen', ' words', ',', ' and', ' to', ' create', ' more', ' compact', ' and', ' consistent', ' representations', ' of', ' the', ' texts', '.', ' B', 'PE', ' can', ' also', ' allow', ' the', ' model', ' to', ' generate', ' new', ' words', ' or', ' tokens', ',', ' by', ' combining', ' existing', ' ones', '.', ' Example', ' text', ' excerpt', ' from', ':', ' https', '://', 'learn', '.microsoft', '.com', '/en', '-us', '/', 'semantic', '-k', 'ernel', '/con', 'cepts', '-', 'ai', '/t', 'okens', ' ']\n",
      "\n",
      "[3404, 2065, 374, 279, 1920, 315, 45473, 279, 1988, 323, 2612, 22755, 1139, 9333, 8316, 430, 649, 387, 15590, 555, 279, 445, 11237, 15592, 4211, 13, 59266, 649, 387, 4339, 11, 5885, 11, 1207, 5880, 11, 477, 18210, 11, 11911, 389, 279, 955, 323, 279, 1404, 315, 279, 1646, 13, 9857, 2065, 649, 1520, 279, 1646, 311, 3790, 2204, 15823, 11, 24757, 1299, 552, 11, 323, 20447, 11, 323, 311, 8108, 279, 55580, 323, 5044, 7194, 13, 9857, 2065, 649, 1101, 7958, 279, 4367, 323, 279, 20057, 315, 279, 8066, 22755, 11, 555, 66700, 279, 7438, 323, 279, 2317, 315, 279, 11460, 13, 9857, 2065, 649, 387, 2884, 1701, 2204, 5528, 11, 1778, 439, 6037, 6108, 11, 29564, 11, 477, 30828, 11, 11911, 389, 279, 23965, 323, 279, 54709, 315, 279, 22755, 13, 220, 5377, 15836, 323, 35219, 5377, 15836, 5829, 264, 1207, 1178, 4037, 2065, 1749, 2663, 330, 7300, 9483, 1334, 30430, 320, 33, 1777, 10143, 369, 1202, 480, 2898, 6108, 4211, 13, 426, 1777, 374, 264, 1749, 430, 82053, 279, 1455, 14134, 31965, 13840, 315, 5885, 477, 5943, 1139, 264, 3254, 4037, 11, 3156, 264, 3738, 1396, 315, 11460, 477, 264, 36018, 1404, 374, 8813, 13, 426, 1777, 649, 1520, 279, 1646, 311, 3790, 9024, 477, 64233, 4339, 11, 323, 311, 1893, 810, 17251, 323, 13263, 44713, 315, 279, 22755, 13, 426, 1777, 649, 1101, 2187, 279, 1646, 311, 7068, 502, 4339, 477, 11460, 11, 555, 35271, 6484, 6305, 13, 13688, 1495, 50565, 505, 25, 3788, 1129, 12964, 28719, 916, 13920, 26766, 14, 48958, 12934, 5571, 32336, 58871, 12, 2192, 5640, 9912, 220]\n",
      "\n",
      "Token Count: 265\n",
      "Characters: 1279\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from IPython.display import clear_output\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "count = 0\n",
    "token_list = []\n",
    "\n",
    "user_input= input(\"\")\n",
    "clear_output(wait=True)\n",
    "\n",
    "encode = tokenizer.encode(user_input)\n",
    "decode = tokenizer.decode_tokens_bytes(encode)\n",
    "\n",
    "for token in decode:\n",
    "    token_list.append(token.decode())\n",
    "\n",
    "character_count = sum(len(i) for i in token_list)\n",
    "length = len(encode)\n",
    "\n",
    "for tk in token_list:\n",
    "    if count == 0:\n",
    "        print('\\x1b[0;47;1m' + tk + '\\x1b[0m', end='')\n",
    "        count+=1\n",
    "    elif count == 1:\n",
    "        print('\\x1b[0;42;1m' + tk + '\\x1b[0m', end='')\n",
    "        count+=1\n",
    "    elif count == 2:\n",
    "        print('\\x1b[0;43;1m' + tk + '\\x1b[0m', end='')\n",
    "        count+=1\n",
    "    elif count == 3:\n",
    "        print('\\x1b[0;44;1m' + tk + '\\x1b[0m', end='')\n",
    "        count+=1\n",
    "    elif count == 4:\n",
    "        print('\\x1b[0;46;1m' + tk + '\\x1b[0m', end='')\n",
    "        count+=1\n",
    "    elif count == 5:\n",
    "        print('\\x1b[0;45;1m' + tk + '\\x1b[0m', end='')\n",
    "        count=0      \n",
    "\n",
    "print(\"\\n\\n\" + str(token_list) +\"\\n\")\n",
    "print(str(encode) + \"\\n\")\n",
    "print(\"Token Count: \" + str(length))\n",
    "print(\"Characters: \" + str(character_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d22e3a2-b549-41c6-bd82-7fb74599f160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;47;1mToken\u001b[0m\u001b[0;42;1mization\u001b[0m\u001b[0;43;1m is\u001b[0m\u001b[0;44;1m the\u001b[0m\u001b[0;46;1m process\u001b[0m\u001b[0;45;1m of\u001b[0m\u001b[0;47;1m splitting\u001b[0m\u001b[0;42;1m the\u001b[0m\u001b[0;43;1m input\u001b[0m\u001b[0;44;1m and\u001b[0m\u001b[0;46;1m output\u001b[0m\u001b[0;45;1m texts\u001b[0m\u001b[0;47;1m into\u001b[0m\u001b[0;42;1m smaller\u001b[0m\u001b[0;43;1m units\u001b[0m\u001b[0;44;1m that\u001b[0m\u001b[0;46;1m can\u001b[0m\u001b[0;45;1m be\u001b[0m\u001b[0;47;1m processed\u001b[0m\u001b[0;42;1m by\u001b[0m\u001b[0;43;1m the\u001b[0m\u001b[0;44;1m L\u001b[0m\u001b[0;46;1mLM\u001b[0m\u001b[0;45;1m AI\u001b[0m\u001b[0;47;1m models\u001b[0m\u001b[0;42;1m.\u001b[0m\u001b[0;43;1m Tokens\u001b[0m\u001b[0;44;1m can\u001b[0m\u001b[0;46;1m be\u001b[0m\u001b[0;45;1m words\u001b[0m\u001b[0;47;1m,\u001b[0m\u001b[0;42;1m characters\u001b[0m\u001b[0;43;1m,\u001b[0m\u001b[0;44;1m sub\u001b[0m\u001b[0;46;1mwords\u001b[0m\u001b[0;45;1m,\u001b[0m\u001b[0;47;1m or\u001b[0m\u001b[0;42;1m symbols\u001b[0m\u001b[0;43;1m,\u001b[0m\u001b[0;44;1m depending\u001b[0m\u001b[0;46;1m on\u001b[0m\u001b[0;45;1m the\u001b[0m\u001b[0;47;1m type\u001b[0m\u001b[0;42;1m and\u001b[0m\u001b[0;43;1m the\u001b[0m\u001b[0;44;1m size\u001b[0m\u001b[0;46;1m of\u001b[0m\u001b[0;45;1m the\u001b[0m\u001b[0;47;1m model\u001b[0m\u001b[0;42;1m.\u001b[0m\u001b[0;43;1m Token\u001b[0m\u001b[0;44;1mization\u001b[0m\u001b[0;46;1m can\u001b[0m\u001b[0;45;1m help\u001b[0m\u001b[0;47;1m the\u001b[0m\u001b[0;42;1m model\u001b[0m\u001b[0;43;1m to\u001b[0m\u001b[0;44;1m handle\u001b[0m\u001b[0;46;1m different\u001b[0m\u001b[0;45;1m languages\u001b[0m\u001b[0;47;1m,\u001b[0m\u001b[0;42;1m vocab\u001b[0m\u001b[0;43;1mular\u001b[0m\u001b[0;44;1mies\u001b[0m\u001b[0;46;1m,\u001b[0m\u001b[0;45;1m and\u001b[0m\u001b[0;47;1m formats\u001b[0m\u001b[0;42;1m,\u001b[0m\u001b[0;43;1m and\u001b[0m\u001b[0;44;1m to\u001b[0m\u001b[0;46;1m reduce\u001b[0m\u001b[0;45;1m the\u001b[0m\u001b[0;47;1m computational\u001b[0m\u001b[0;42;1m and\u001b[0m\u001b[0;43;1m memory\u001b[0m\u001b[0;44;1m costs\u001b[0m\u001b[0;46;1m.\u001b[0m\u001b[0;45;1m Token\u001b[0m\u001b[0;47;1mization\u001b[0m\u001b[0;42;1m can\u001b[0m\u001b[0;43;1m also\u001b[0m\u001b[0;44;1m affect\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m quality\u001b[0m\u001b[0;47;1m and\u001b[0m\u001b[0;42;1m the\u001b[0m\u001b[0;43;1m diversity\u001b[0m\u001b[0;44;1m of\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m generated\u001b[0m\u001b[0;47;1m texts\u001b[0m\u001b[0;42;1m,\u001b[0m\u001b[0;43;1m by\u001b[0m\u001b[0;44;1m influencing\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m meaning\u001b[0m\u001b[0;47;1m and\u001b[0m\u001b[0;42;1m the\u001b[0m\u001b[0;43;1m context\u001b[0m\u001b[0;44;1m of\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m tokens\u001b[0m\u001b[0;47;1m.\u001b[0m\u001b[0;42;1m Token\u001b[0m\u001b[0;43;1mization\u001b[0m\u001b[0;44;1m can\u001b[0m\u001b[0;46;1m be\u001b[0m\u001b[0;45;1m done\u001b[0m\u001b[0;47;1m using\u001b[0m\u001b[0;42;1m different\u001b[0m\u001b[0;43;1m methods\u001b[0m\u001b[0;44;1m,\u001b[0m\u001b[0;46;1m such\u001b[0m\u001b[0;45;1m as\u001b[0m\u001b[0;47;1m rule\u001b[0m\u001b[0;42;1m-based\u001b[0m\u001b[0;43;1m,\u001b[0m\u001b[0;44;1m statistical\u001b[0m\u001b[0;46;1m,\u001b[0m\u001b[0;45;1m or\u001b[0m\u001b[0;47;1m neural\u001b[0m\u001b[0;42;1m,\u001b[0m\u001b[0;43;1m depending\u001b[0m\u001b[0;44;1m on\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m complexity\u001b[0m\u001b[0;47;1m and\u001b[0m\u001b[0;42;1m the\u001b[0m\u001b[0;43;1m variability\u001b[0m\u001b[0;44;1m of\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m texts\u001b[0m\u001b[0;47;1m.\u001b[0m\u001b[0;42;1m \u001b[0m\u001b[0;43;1m Open\u001b[0m\u001b[0;44;1mAI\u001b[0m\u001b[0;46;1m and\u001b[0m\u001b[0;45;1m Azure\u001b[0m\u001b[0;47;1m Open\u001b[0m\u001b[0;42;1mAI\u001b[0m\u001b[0;43;1m uses\u001b[0m\u001b[0;44;1m a\u001b[0m\u001b[0;46;1m sub\u001b[0m\u001b[0;45;1mword\u001b[0m\u001b[0;47;1m token\u001b[0m\u001b[0;42;1mization\u001b[0m\u001b[0;43;1m method\u001b[0m\u001b[0;44;1m called\u001b[0m\u001b[0;46;1m \"\u001b[0m\u001b[0;45;1mByte\u001b[0m\u001b[0;47;1m-P\u001b[0m\u001b[0;42;1mair\u001b[0m\u001b[0;43;1m Encoding\u001b[0m\u001b[0;44;1m (\u001b[0m\u001b[0;46;1mB\u001b[0m\u001b[0;45;1mPE\u001b[0m\u001b[0;47;1m)\"\u001b[0m\u001b[0;42;1m for\u001b[0m\u001b[0;43;1m its\u001b[0m\u001b[0;44;1m G\u001b[0m\u001b[0;46;1mPT\u001b[0m\u001b[0;45;1m-based\u001b[0m\u001b[0;47;1m models\u001b[0m\u001b[0;42;1m.\u001b[0m\u001b[0;43;1m B\u001b[0m\u001b[0;44;1mPE\u001b[0m\u001b[0;46;1m is\u001b[0m\u001b[0;45;1m a\u001b[0m\u001b[0;47;1m method\u001b[0m\u001b[0;42;1m that\u001b[0m\u001b[0;43;1m merges\u001b[0m\u001b[0;44;1m the\u001b[0m\u001b[0;46;1m most\u001b[0m\u001b[0;45;1m frequently\u001b[0m\u001b[0;47;1m occurring\u001b[0m\u001b[0;42;1m pairs\u001b[0m\u001b[0;43;1m of\u001b[0m\u001b[0;44;1m characters\u001b[0m\u001b[0;46;1m or\u001b[0m\u001b[0;45;1m bytes\u001b[0m\u001b[0;47;1m into\u001b[0m\u001b[0;42;1m a\u001b[0m\u001b[0;43;1m single\u001b[0m\u001b[0;44;1m token\u001b[0m\u001b[0;46;1m,\u001b[0m\u001b[0;45;1m until\u001b[0m\u001b[0;47;1m a\u001b[0m\u001b[0;42;1m certain\u001b[0m\u001b[0;43;1m number\u001b[0m\u001b[0;44;1m of\u001b[0m\u001b[0;46;1m tokens\u001b[0m\u001b[0;45;1m or\u001b[0m\u001b[0;47;1m a\u001b[0m\u001b[0;42;1m vocabulary\u001b[0m\u001b[0;43;1m size\u001b[0m\u001b[0;44;1m is\u001b[0m\u001b[0;46;1m reached\u001b[0m\u001b[0;45;1m.\u001b[0m\u001b[0;47;1m B\u001b[0m\u001b[0;42;1mPE\u001b[0m\u001b[0;43;1m can\u001b[0m\u001b[0;44;1m help\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m model\u001b[0m\u001b[0;47;1m to\u001b[0m\u001b[0;42;1m handle\u001b[0m\u001b[0;43;1m rare\u001b[0m\u001b[0;44;1m or\u001b[0m\u001b[0;46;1m unseen\u001b[0m\u001b[0;45;1m words\u001b[0m\u001b[0;47;1m,\u001b[0m\u001b[0;42;1m and\u001b[0m\u001b[0;43;1m to\u001b[0m\u001b[0;44;1m create\u001b[0m\u001b[0;46;1m more\u001b[0m\u001b[0;45;1m compact\u001b[0m\u001b[0;47;1m and\u001b[0m\u001b[0;42;1m consistent\u001b[0m\u001b[0;43;1m representations\u001b[0m\u001b[0;44;1m of\u001b[0m\u001b[0;46;1m the\u001b[0m\u001b[0;45;1m texts\u001b[0m\u001b[0;47;1m.\u001b[0m\u001b[0;42;1m B\u001b[0m\u001b[0;43;1mPE\u001b[0m\u001b[0;44;1m can\u001b[0m\u001b[0;46;1m also\u001b[0m\u001b[0;45;1m allow\u001b[0m\u001b[0;47;1m the\u001b[0m\u001b[0;42;1m model\u001b[0m\u001b[0;43;1m to\u001b[0m\u001b[0;44;1m generate\u001b[0m\u001b[0;46;1m new\u001b[0m\u001b[0;45;1m words\u001b[0m\u001b[0;47;1m or\u001b[0m\u001b[0;42;1m tokens\u001b[0m\u001b[0;43;1m,\u001b[0m\u001b[0;44;1m by\u001b[0m\u001b[0;46;1m combining\u001b[0m\u001b[0;45;1m existing\u001b[0m\u001b[0;47;1m ones\u001b[0m\u001b[0;42;1m.\u001b[0m\u001b[0;43;1m Example\u001b[0m\u001b[0;44;1m text\u001b[0m\u001b[0;46;1m excerpt\u001b[0m\u001b[0;45;1m from\u001b[0m\u001b[0;47;1m:\u001b[0m\u001b[0;42;1m https\u001b[0m\u001b[0;43;1m://\u001b[0m\u001b[0;44;1mlearn\u001b[0m\u001b[0;46;1m.microsoft\u001b[0m\u001b[0;45;1m.com\u001b[0m\u001b[0;47;1m/en\u001b[0m\u001b[0;42;1m-us\u001b[0m\u001b[0;43;1m/\u001b[0m\u001b[0;44;1msemantic\u001b[0m\u001b[0;46;1m-k\u001b[0m\u001b[0;45;1mernel\u001b[0m\u001b[0;47;1m/con\u001b[0m\u001b[0;42;1mcepts\u001b[0m\u001b[0;43;1m-\u001b[0m\u001b[0;44;1mai\u001b[0m\u001b[0;46;1m/t\u001b[0m\u001b[0;45;1mokens\u001b[0m\u001b[0;47;1m \u001b[0m\n",
      "\n",
      "['Token', 'ization', ' is', ' the', ' process', ' of', ' splitting', ' the', ' input', ' and', ' output', ' texts', ' into', ' smaller', ' units', ' that', ' can', ' be', ' processed', ' by', ' the', ' L', 'LM', ' AI', ' models', '.', ' Tokens', ' can', ' be', ' words', ',', ' characters', ',', ' sub', 'words', ',', ' or', ' symbols', ',', ' depending', ' on', ' the', ' type', ' and', ' the', ' size', ' of', ' the', ' model', '.', ' Token', 'ization', ' can', ' help', ' the', ' model', ' to', ' handle', ' different', ' languages', ',', ' vocab', 'ular', 'ies', ',', ' and', ' formats', ',', ' and', ' to', ' reduce', ' the', ' computational', ' and', ' memory', ' costs', '.', ' Token', 'ization', ' can', ' also', ' affect', ' the', ' quality', ' and', ' the', ' diversity', ' of', ' the', ' generated', ' texts', ',', ' by', ' influencing', ' the', ' meaning', ' and', ' the', ' context', ' of', ' the', ' tokens', '.', ' Token', 'ization', ' can', ' be', ' done', ' using', ' different', ' methods', ',', ' such', ' as', ' rule', '-based', ',', ' statistical', ',', ' or', ' neural', ',', ' depending', ' on', ' the', ' complexity', ' and', ' the', ' variability', ' of', ' the', ' texts', '.', ' ', ' Open', 'AI', ' and', ' Azure', ' Open', 'AI', ' uses', ' a', ' sub', 'word', ' token', 'ization', ' method', ' called', ' \"', 'Byte', '-P', 'air', ' Encoding', ' (', 'B', 'PE', ')\"', ' for', ' its', ' G', 'PT', '-based', ' models', '.', ' B', 'PE', ' is', ' a', ' method', ' that', ' merges', ' the', ' most', ' frequently', ' occurring', ' pairs', ' of', ' characters', ' or', ' bytes', ' into', ' a', ' single', ' token', ',', ' until', ' a', ' certain', ' number', ' of', ' tokens', ' or', ' a', ' vocabulary', ' size', ' is', ' reached', '.', ' B', 'PE', ' can', ' help', ' the', ' model', ' to', ' handle', ' rare', ' or', ' unseen', ' words', ',', ' and', ' to', ' create', ' more', ' compact', ' and', ' consistent', ' representations', ' of', ' the', ' texts', '.', ' B', 'PE', ' can', ' also', ' allow', ' the', ' model', ' to', ' generate', ' new', ' words', ' or', ' tokens', ',', ' by', ' combining', ' existing', ' ones', '.', ' Example', ' text', ' excerpt', ' from', ':', ' https', '://', 'learn', '.microsoft', '.com', '/en', '-us', '/', 'semantic', '-k', 'ernel', '/con', 'cepts', '-', 'ai', '/t', 'okens', ' ']\n",
      "\n",
      "[3404, 2065, 374, 279, 1920, 315, 45473, 279, 1988, 323, 2612, 22755, 1139, 9333, 8316, 430, 649, 387, 15590, 555, 279, 445, 11237, 15592, 4211, 13, 59266, 649, 387, 4339, 11, 5885, 11, 1207, 5880, 11, 477, 18210, 11, 11911, 389, 279, 955, 323, 279, 1404, 315, 279, 1646, 13, 9857, 2065, 649, 1520, 279, 1646, 311, 3790, 2204, 15823, 11, 24757, 1299, 552, 11, 323, 20447, 11, 323, 311, 8108, 279, 55580, 323, 5044, 7194, 13, 9857, 2065, 649, 1101, 7958, 279, 4367, 323, 279, 20057, 315, 279, 8066, 22755, 11, 555, 66700, 279, 7438, 323, 279, 2317, 315, 279, 11460, 13, 9857, 2065, 649, 387, 2884, 1701, 2204, 5528, 11, 1778, 439, 6037, 6108, 11, 29564, 11, 477, 30828, 11, 11911, 389, 279, 23965, 323, 279, 54709, 315, 279, 22755, 13, 220, 5377, 15836, 323, 35219, 5377, 15836, 5829, 264, 1207, 1178, 4037, 2065, 1749, 2663, 330, 7300, 9483, 1334, 30430, 320, 33, 1777, 10143, 369, 1202, 480, 2898, 6108, 4211, 13, 426, 1777, 374, 264, 1749, 430, 82053, 279, 1455, 14134, 31965, 13840, 315, 5885, 477, 5943, 1139, 264, 3254, 4037, 11, 3156, 264, 3738, 1396, 315, 11460, 477, 264, 36018, 1404, 374, 8813, 13, 426, 1777, 649, 1520, 279, 1646, 311, 3790, 9024, 477, 64233, 4339, 11, 323, 311, 1893, 810, 17251, 323, 13263, 44713, 315, 279, 22755, 13, 426, 1777, 649, 1101, 2187, 279, 1646, 311, 7068, 502, 4339, 477, 11460, 11, 555, 35271, 6484, 6305, 13, 13688, 1495, 50565, 505, 25, 3788, 1129, 12964, 28719, 916, 13920, 26766, 14, 48958, 12934, 5571, 32336, 58871, 12, 2192, 5640, 9912, 220]\n",
      "\n",
      "Token Count: 265\n",
      "Characters: 1279\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from IPython.display import clear_output\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "color_codes = ['0;47', '0;42', '0;43', '0;44', '0;46', '0;45']\n",
    "\n",
    "user_input = input(\"\")\n",
    "clear_output(wait=True)\n",
    "\n",
    "encoded = tokenizer.encode(user_input)\n",
    "decoded = tokenizer.decode_tokens_bytes(encoded)\n",
    "token_list = [token.decode() for token in decoded]\n",
    "\n",
    "character_count = sum(len(i) for i in token_list)\n",
    "\n",
    "for idx, token in enumerate(token_list):\n",
    "    print(f'\\x1b[{color_codes[idx % len(color_codes)]};1m{token}\\x1b[0m', end='')\n",
    "\n",
    "print(\"\\n\\n\" + str(token_list) + \"\\n\")\n",
    "print(str(encoded) + \"\\n\")\n",
    "print(\"Token Count: \" + str(len(encoded)))\n",
    "print(\"Characters: \" + str(character_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173adf49-4f15-448a-8f22-283394fdda0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
